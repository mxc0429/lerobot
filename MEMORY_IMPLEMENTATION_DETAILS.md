# SmolVLA è®°å¿†æ¨¡å—å®ç°è¯¦è§£

## ğŸ“‹ ç›®å½•
1. [ä¿®æ”¹çš„æ¨¡å‹ç»“æ„](#ä¿®æ”¹çš„æ¨¡å‹ç»“æ„)
2. [è®°å¿†æ¨¡å—å®ç°åŸç†](#è®°å¿†æ¨¡å—å®ç°åŸç†)
3. [ä¸ LM-RMT çš„å¯¹æ¯”](#ä¸-lm-rmt-çš„å¯¹æ¯”)
4. [ä»£ç è¯¦è§£](#ä»£ç è¯¦è§£)

---

## 1. ä¿®æ”¹çš„æ¨¡å‹ç»“æ„

### 1.1 é…ç½®æ–‡ä»¶ä¿®æ”¹ (`configuration_smolvla.py`)

```python
# æ–°å¢çš„é…ç½®å‚æ•°
class SmolVLAConfig(PreTrainedConfig):
    # ... åŸæœ‰é…ç½® ...
    
    # RMT Memory settings (æ–°å¢)
    num_mem_tokens: int = 0          # è®°å¿† token æ•°é‡
    mem_at_end: bool = False         # æ˜¯å¦åœ¨åºåˆ—æœ«å°¾æ·»åŠ è®°å¿†
    read_mem_from_cache: bool = False # æ˜¯å¦ä»ç¼“å­˜è¯»å–è®°å¿†
```

**ä½œç”¨**: æ§åˆ¶è®°å¿†æ¨¡å—çš„è¡Œä¸ºï¼Œé»˜è®¤ç¦ç”¨ï¼ˆå‘åå…¼å®¹ï¼‰ã€‚

---

### 1.2 æ¨¡å‹æ–‡ä»¶ä¿®æ”¹ (`modeling_smolvla.py`)

#### ä¿®æ”¹ç‚¹ 1: VLAFlowMatching ç±»åˆå§‹åŒ–

```python
class VLAFlowMatching(nn.Module):
    def __init__(self, config: SmolVLAConfig):
        super().__init__()
        # ... åŸæœ‰åˆå§‹åŒ– ...
        
        # ğŸ†• åˆå§‹åŒ–è®°å¿† tokens
        self.init_mem_tokens()
    
    def init_mem_tokens(self):
        """åˆå§‹åŒ–å¯å­¦ä¹ çš„è®°å¿† tokens"""
        if self.config.num_mem_tokens == 0:
            self.mem_tokens = None
        else:
            hidden_size = self.vlm_with_expert.config.text_config.hidden_size  # 960
            # åˆ›å»ºå¯å­¦ä¹ å‚æ•°: [num_mem_tokens, 1, hidden_size]
            mem_tokens = torch.randn(self.config.num_mem_tokens, 1, hidden_size) * 0.02
            self.mem_tokens = nn.Parameter(mem_tokens, requires_grad=True)
```

**å…³é”®ç‚¹**:
- è®°å¿† tokens æ˜¯**å¯å­¦ä¹ çš„å‚æ•°** (`nn.Parameter`)
- å½¢çŠ¶: `[num_mem_tokens, 1, hidden_size]`
- å°éšæœºåˆå§‹åŒ– (std=0.02) é¿å…å½±å“è®­ç»ƒç¨³å®šæ€§

---

#### ä¿®æ”¹ç‚¹ 2: embed_prefix æ–¹æ³•

**åŸå§‹ä»£ç **:
```python
def embed_prefix(self, images, img_masks, lang_tokens, lang_masks, state):
    embs = []
    # æ·»åŠ å›¾åƒåµŒå…¥
    # æ·»åŠ è¯­è¨€åµŒå…¥
    # æ·»åŠ çŠ¶æ€åµŒå…¥
    return torch.cat(embs, dim=1), pad_masks, att_masks
```

**ä¿®æ”¹å**:
```python
def embed_prefix(self, images, img_masks, lang_tokens, lang_masks, state, mem_tokens=None):
    embs = []
    pad_masks = []
    att_masks = []
    
    # ğŸ†• 1. åœ¨åºåˆ—å¼€å¤´æ·»åŠ è®°å¿† tokens
    if self.config.num_mem_tokens > 0:
        if mem_tokens is None:
            # ä½¿ç”¨åˆå§‹åŒ–çš„è®°å¿† tokens
            mem_emb = self.mem_tokens.expand(-1, batch_size, -1)
        else:
            # ä½¿ç”¨ä¸Šä¸€æ—¶é—´æ­¥ä¼ æ¥çš„è®°å¿† tokens
            mem_emb = mem_tokens
        
        mem_emb = mem_emb.transpose(0, 1)  # [batch, num_mem, hidden]
        embs.append(mem_emb)
        
        # è®°å¿† tokens çš„æ©ç 
        mem_mask = torch.ones(batch_size, self.config.num_mem_tokens, dtype=torch.bool)
        pad_masks.append(mem_mask)
        
        # è®°å¿† tokens å¯ä»¥äº’ç›¸æ³¨æ„ (att_mask=0)
        att_masks += [0] * self.config.num_mem_tokens
    
    # 2. æ·»åŠ å›¾åƒåµŒå…¥ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
    # 3. æ·»åŠ è¯­è¨€åµŒå…¥ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
    # 4. æ·»åŠ çŠ¶æ€åµŒå…¥ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
    
    # ğŸ†• 5. å¯é€‰ï¼šåœ¨åºåˆ—æœ«å°¾ä¹Ÿæ·»åŠ è®°å¿† tokens
    if self.config.num_mem_tokens > 0 and self.config.mem_at_end:
        # ... ç±»ä¼¼é€»è¾‘ ...
    
    return torch.cat(embs, dim=1), torch.cat(pad_masks, dim=1), att_masks
```

**åºåˆ—ç»“æ„å˜åŒ–**:
```
åŸå§‹: [images] + [language] + [state] + [actions]
ä¿®æ”¹: [memory] + [images] + [language] + [state] + [actions]
```

---

#### ä¿®æ”¹ç‚¹ 3: forward æ–¹æ³•

**åŸå§‹ä»£ç **:
```python
def forward(self, images, img_masks, lang_tokens, lang_masks, state, actions):
    # ... å‰å‘ä¼ æ’­ ...
    losses = F.mse_loss(u_t, v_t, reduction="none")
    return losses
```

**ä¿®æ”¹å**:
```python
def forward(self, images, img_masks, lang_tokens, lang_masks, state, actions, 
            noise=None, time=None, mem_tokens=None):
    # 1. åµŒå…¥å‰ç¼€ï¼ˆåŒ…å«è®°å¿† tokensï¼‰
    prefix_embs, prefix_pad_masks, prefix_att_masks = self.embed_prefix(
        images, img_masks, lang_tokens, lang_masks, state=state, 
        mem_tokens=mem_tokens  # ğŸ†• ä¼ å…¥è®°å¿†
    )
    
    # 2. å‰å‘ä¼ æ’­
    (prefix_out, suffix_out), _ = self.vlm_with_expert.forward(...)
    
    # ğŸ†• 3. æå–æ›´æ–°åçš„è®°å¿† tokens
    updated_mem_tokens = None
    if self.config.num_mem_tokens > 0:
        if self.config.mem_at_end:
            # ä»åºåˆ—æœ«å°¾æå–
            updated_mem_tokens = prefix_out[:, -self.config.num_mem_tokens:, :]
        else:
            # ä»åºåˆ—å¼€å¤´æå–
            updated_mem_tokens = prefix_out[:, :self.config.num_mem_tokens, :]
        updated_mem_tokens = updated_mem_tokens.transpose(0, 1)  # [num_mem, batch, hidden]
    
    # 4. è®¡ç®—æŸå¤±
    losses = F.mse_loss(u_t, v_t, reduction="none")
    
    # ğŸ†• 5. è¿”å›æŸå¤±å’Œæ›´æ–°çš„è®°å¿†
    return losses, updated_mem_tokens
```

**å…³é”®å˜åŒ–**:
- è¾“å…¥å¢åŠ  `mem_tokens` å‚æ•°
- è¾“å‡ºå¢åŠ  `updated_mem_tokens`
- è®°å¿†åœ¨ Transformer å¤„ç†åè¢«æ›´æ–°

---

#### ä¿®æ”¹ç‚¹ 4: sample_actions æ–¹æ³•

**åŸå§‹ä»£ç **:
```python
def sample_actions(self, images, img_masks, lang_tokens, lang_masks, state):
    # ... æ¨ç†é€»è¾‘ ...
    return actions
```

**ä¿®æ”¹å**:
```python
def sample_actions(self, images, img_masks, lang_tokens, lang_masks, state, 
                   noise=None, mem_tokens=None):
    # 1. åµŒå…¥å‰ç¼€ï¼ˆåŒ…å«è®°å¿†ï¼‰
    prefix_embs, prefix_pad_masks, prefix_att_masks = self.embed_prefix(
        images, img_masks, lang_tokens, lang_masks, state=state, 
        mem_tokens=mem_tokens  # ğŸ†•
    )
    
    # 2. è®¡ç®— KV cache
    prefix_out, past_key_values = self.vlm_with_expert.forward(...)
    
    # ğŸ†• 3. æå–æ›´æ–°åçš„è®°å¿†
    updated_mem_tokens = None
    if self.config.num_mem_tokens > 0:
        prefix_out = prefix_out[0]
        if self.config.mem_at_end:
            updated_mem_tokens = prefix_out[:, -self.config.num_mem_tokens:, :]
        else:
            updated_mem_tokens = prefix_out[:, :self.config.num_mem_tokens, :]
        updated_mem_tokens = updated_mem_tokens.transpose(0, 1)
    
    # 4. å»å™ªé‡‡æ ·
    # ... Flow Matching é‡‡æ ·é€»è¾‘ ...
    
    # ğŸ†• 5. è¿”å›åŠ¨ä½œå’Œæ›´æ–°çš„è®°å¿†
    return actions, updated_mem_tokens
```

---

#### ä¿®æ”¹ç‚¹ 5: SmolVLAPolicy ç±»

**åŸå§‹ä»£ç **:
```python
class SmolVLAPolicy(PreTrainedPolicy):
    def __init__(self, config):
        super().__init__(config)
        self.model = VLAFlowMatching(config)
        self.reset()
    
    def reset(self):
        self._queues = {ACTION: deque(maxlen=self.config.n_action_steps)}
```

**ä¿®æ”¹å**:
```python
class SmolVLAPolicy(PreTrainedPolicy):
    def __init__(self, config):
        super().__init__(config)
        self.model = VLAFlowMatching(config)
        self.reset()
        # ğŸ†• åˆå§‹åŒ–è®°å¿†çŠ¶æ€å˜é‡
        self._mem_tokens_state = None
    
    def reset(self):
        self._queues = {ACTION: deque(maxlen=self.config.n_action_steps)}
        # ğŸ†• é‡ç½®è®°å¿†çŠ¶æ€
        self._mem_tokens_state = None
```

---

#### ä¿®æ”¹ç‚¹ 6: _get_action_chunk æ–¹æ³•

**åŸå§‹ä»£ç **:
```python
def _get_action_chunk(self, batch, noise=None):
    # ... å‡†å¤‡è¾“å…¥ ...
    actions = self.model.sample_actions(images, img_masks, lang_tokens, lang_masks, state)
    return actions
```

**ä¿®æ”¹å**:
```python
def _get_action_chunk(self, batch, noise=None):
    # ... å‡†å¤‡è¾“å…¥ ...
    
    # ğŸ†• ä¼ å…¥è®°å¿†çŠ¶æ€ï¼Œè·å–æ›´æ–°çš„è®°å¿†
    actions, updated_mem_tokens = self.model.sample_actions(
        images, img_masks, lang_tokens, lang_masks, state, 
        noise=noise, mem_tokens=self._mem_tokens_state
    )
    
    # ğŸ†• æ›´æ–°è®°å¿†çŠ¶æ€ä¾›ä¸‹ä¸€æ—¶é—´æ­¥ä½¿ç”¨
    if self.config.num_mem_tokens > 0:
        self._mem_tokens_state = updated_mem_tokens.detach()
    
    return actions
```

**å…³é”®ç‚¹**:
- è®°å¿†çŠ¶æ€åœ¨æ—¶é—´æ­¥ä¹‹é—´ä¼ é€’
- ä½¿ç”¨ `.detach()` é¿å…æ¢¯åº¦ç´¯ç§¯

---

## 2. è®°å¿†æ¨¡å—å®ç°åŸç†

### 2.1 è®°å¿†æµè½¬è¿‡ç¨‹

```
æ—¶é—´æ­¥ t=0:
  åˆå§‹åŒ–: mem_tokens = éšæœºåˆå§‹åŒ–çš„å¯å­¦ä¹ å‚æ•°
  è¾“å…¥: [mem_tokens] + [obs_0] + [lang] + [state_0]
    â†“ Transformer
  è¾“å‡º: [mem_tokens_0'] + [action_0]
  ä¿å­˜: mem_tokens_state = mem_tokens_0'

æ—¶é—´æ­¥ t=1:
  è¾“å…¥: [mem_tokens_0'] + [obs_1] + [lang] + [state_1]
    â†“ Transformer
  è¾“å‡º: [mem_tokens_1'] + [action_1]
  ä¿å­˜: mem_tokens_state = mem_tokens_1'

æ—¶é—´æ­¥ t=2:
  è¾“å…¥: [mem_tokens_1'] + [obs_2] + [lang] + [state_2]
    â†“ Transformer
  è¾“å‡º: [mem_tokens_2'] + [action_1]
  ä¿å­˜: mem_tokens_state = mem_tokens_2'

...
```

### 2.2 æ³¨æ„åŠ›æœºåˆ¶

```python
# æ³¨æ„åŠ›æ©ç è®¾ç½®
att_masks = []

# è®°å¿† tokens å¯ä»¥äº’ç›¸æ³¨æ„ï¼Œä¹Ÿå¯ä»¥æ³¨æ„åç»­ tokens
att_masks += [0] * num_mem_tokens  # 0 = å¯ä»¥æ³¨æ„

# å›¾åƒã€è¯­è¨€ tokens å¯ä»¥æ³¨æ„è®°å¿†å’Œå½¼æ­¤
att_masks += [0] * num_image_tokens
att_masks += [0] * num_lang_tokens

# çŠ¶æ€å’ŒåŠ¨ä½œ tokens ä¸èƒ½è¢«å‰é¢çš„ tokens æ³¨æ„
att_masks += [1] * num_state_tokens  # 1 = ä¸èƒ½æ³¨æ„
att_masks += [1] * num_action_tokens
```

**æ³¨æ„åŠ›çŸ©é˜µ**:
```
         mem  img  lang  state  action
mem      âœ“    âœ“    âœ“     âœ“      âœ“
img      âœ“    âœ“    âœ“     âœ“      âœ“
lang     âœ“    âœ“    âœ“     âœ“      âœ“
state    âœ—    âœ—    âœ—     âœ“      âœ“
action   âœ—    âœ—    âœ—     âœ—      âœ“
```

### 2.3 è®­ç»ƒ vs æ¨ç†

**è®­ç»ƒæ—¶**:
- æ¯ä¸ªæ ·æœ¬çš„è®°å¿†æ˜¯ç‹¬ç«‹çš„
- ä¸è·¨ batch ä¿æŒè®°å¿†
- è®°å¿† tokens é€šè¿‡åå‘ä¼ æ’­å­¦ä¹ 

**æ¨ç†æ—¶**:
- è®°å¿†åœ¨æ—¶é—´æ­¥ä¹‹é—´æŒä¹…åŒ–
- Episode å¼€å§‹æ—¶è°ƒç”¨ `reset()` æ¸…ç©ºè®°å¿†
- è®°å¿†çŠ¶æ€å­˜å‚¨åœ¨ `_mem_tokens_state`

---

## 3. ä¸ LM-RMT çš„å¯¹æ¯”

### 3.1 ç›¸ä¼¼ä¹‹å¤„

| ç‰¹æ€§ | LM-RMT | SmolVLA Memory |
|------|--------|----------------|
| **æ ¸å¿ƒæ€æƒ³** | å¯å­¦ä¹ çš„è®°å¿† tokens | âœ“ ç›¸åŒ |
| **å³æ’å³ç”¨** | ä¸ä¿®æ”¹ Transformer | âœ“ ç›¸åŒ |
| **è®°å¿†ä½ç½®** | åºåˆ—å¼€å¤´/æœ«å°¾ | âœ“ ç›¸åŒ |
| **å¯å­¦ä¹ æ€§** | nn.Parameter | âœ“ ç›¸åŒ |
| **è½»é‡çº§** | å‚æ•°å¢é‡ < 0.01% | âœ“ ç›¸åŒ |

### 3.2 å…³é”®åŒºåˆ«

#### åŒºåˆ« 1: åº”ç”¨åœºæ™¯

**LM-RMT**:
```python
# è¯­è¨€å»ºæ¨¡ï¼šå¤„ç†é•¿æ–‡æœ¬
è¾“å…¥: æ–‡æœ¬ segment 1 â†’ è¾“å‡º + è®°å¿†
è¾“å…¥: è®°å¿† + æ–‡æœ¬ segment 2 â†’ è¾“å‡º + è®°å¿†
è¾“å…¥: è®°å¿† + æ–‡æœ¬ segment 3 â†’ è¾“å‡º + è®°å¿†
```

**SmolVLA Memory**:
```python
# æœºå™¨äººæ§åˆ¶ï¼šå¤„ç†æ—¶é—´åºåˆ—
è¾“å…¥: è®°å¿† + è§‚å¯Ÿ t=0 â†’ åŠ¨ä½œ + è®°å¿†
è¾“å…¥: è®°å¿† + è§‚å¯Ÿ t=1 â†’ åŠ¨ä½œ + è®°å¿†
è¾“å…¥: è®°å¿† + è§‚å¯Ÿ t=2 â†’ åŠ¨ä½œ + è®°å¿†
```

---

#### åŒºåˆ« 2: è¾“å…¥ç»“æ„

**LM-RMT**:
```python
# çº¯æ–‡æœ¬è¾“å…¥
input_sequence = [mem_tokens] + [text_tokens]
```

**SmolVLA Memory**:
```python
# å¤šæ¨¡æ€è¾“å…¥
input_sequence = [mem_tokens] + [image_tokens] + [language_tokens] + [state_tokens]
```

---

#### åŒºåˆ« 3: è®°å¿†æ›´æ–°æœºåˆ¶

**LM-RMT**:
```python
# åœ¨ forward ä¸­ç›´æ¥å¤„ç†è®°å¿†
def forward(self, data, target, *mems):
    # æ·»åŠ è®°å¿†åˆ°è¾“å…¥
    word_emb = torch.cat([mem_tokens, word_emb], dim=0)
    
    # Transformer å¤„ç†
    hidden = self.transformer(word_emb)
    
    # æå–æ›´æ–°çš„è®°å¿†
    mem_tokens_write = hidden[-num_mem:]
    
    # è¿”å›
    return [mem_tokens_write, loss] + new_mems
```

**SmolVLA Memory**:
```python
# åœ¨ Policy å±‚ç®¡ç†è®°å¿†çŠ¶æ€
def _get_action_chunk(self, batch):
    # ä» Policy çŠ¶æ€è·å–è®°å¿†
    mem_tokens = self._mem_tokens_state
    
    # è°ƒç”¨æ¨¡å‹
    actions, updated_mem = self.model.sample_actions(..., mem_tokens=mem_tokens)
    
    # æ›´æ–° Policy çŠ¶æ€
    self._mem_tokens_state = updated_mem.detach()
    
    return actions
```

---

#### åŒºåˆ« 4: æ³¨æ„åŠ›æ©ç 

**LM-RMT**:
```python
# å› æœæ³¨æ„åŠ› + è®°å¿†ç‰¹æ®Šè§„åˆ™
if self.num_mem_tokens != 0:
    # è®°å¿† tokens å¯ä»¥äº’ç›¸æ³¨æ„
    dec_attn_mask[:num_mem, :num_mem] = 0
    # è®°å¿† tokens æ˜¯å¦ä»ç¼“å­˜è¯»å–
    dec_attn_mask[:num_mem, :mlen] = 1 - int(self.read_mem_from_cache)
```

**SmolVLA Memory**:
```python
# å‰ç¼€-åç¼€æ³¨æ„åŠ› + è®°å¿†è§„åˆ™
# è®°å¿†ã€å›¾åƒã€è¯­è¨€å¯ä»¥äº’ç›¸æ³¨æ„
att_masks += [0] * (num_mem + num_img + num_lang)
# çŠ¶æ€å’ŒåŠ¨ä½œä¸èƒ½è¢«å‰é¢æ³¨æ„
att_masks += [1] * (num_state + num_action)
```

---

#### åŒºåˆ« 5: ä¸å…¶ä»–æœºåˆ¶çš„é›†æˆ

**LM-RMT**:
```python
# å¯ä»¥ä¸ Transformer-XL çš„ mem_len ç»“åˆ
# mem_len: ç¼“å­˜çš„å†å² hidden states
# mem_tokens: å¯å­¦ä¹ çš„è®°å¿† tokens
```

**SmolVLA Memory**:
```python
# ä¸ Flow Matching é›†æˆ
# è®°å¿†å½±å“å»å™ªè¿‡ç¨‹
# ä¸ KV Cache é›†æˆï¼ˆæ¨ç†åŠ é€Ÿï¼‰
```

---

#### åŒºåˆ« 6: è®­ç»ƒç­–ç•¥

**LM-RMT**:
```python
# å¯é€‰ï¼šè·¨ segment åå‘ä¼ æ’­
if mem_backprop_depth > 0:
    # æ¢¯åº¦ä¼ æ’­åˆ°è¿‡å»çš„ segments
    pass
```

**SmolVLA Memory**:
```python
# è®­ç»ƒæ—¶æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹
# æ¨ç†æ—¶è®°å¿†æŒä¹…åŒ–
# ä½¿ç”¨ .detach() é¿å…æ¢¯åº¦ç´¯ç§¯
```

---

### 3.3 æ¶æ„å¯¹æ¯”å›¾

**LM-RMT æ¶æ„**:
```
Segment 1:
[mem_init] + [text_1] â†’ Transformer â†’ [mem_1] + [pred_1]
                                           â†“
Segment 2:                                 â†“
[mem_1] + [text_2] â†’ Transformer â†’ [mem_2] + [pred_2]
                                        â†“
Segment 3:                              â†“
[mem_2] + [text_3] â†’ Transformer â†’ [mem_3] + [pred_3]
```

**SmolVLA Memory æ¶æ„**:
```
Time t=0:
[mem_init] + [img_0, lang, state_0] â†’ VLM + Expert â†’ [mem_0] + [action_0]
                                                          â†“
Time t=1:                                                 â†“
[mem_0] + [img_1, lang, state_1] â†’ VLM + Expert â†’ [mem_1] + [action_1]
                                                       â†“
Time t=2:                                              â†“
[mem_1] + [img_2, lang, state_2] â†’ VLM + Expert â†’ [mem_2] + [action_2]
```

---

## 4. ä»£ç è¯¦è§£

### 4.1 è®°å¿†åˆå§‹åŒ–

```python
def init_mem_tokens(self):
    """åˆå§‹åŒ–å¯å­¦ä¹ çš„è®°å¿† tokens"""
    if self.config.num_mem_tokens == 0:
        self.mem_tokens = None
    else:
        # VLM çš„éšè—å±‚ç»´åº¦ï¼ˆSmolVLM2-500M æ˜¯ 960ï¼‰
        hidden_size = self.vlm_with_expert.config.text_config.hidden_size
        
        # åˆ›å»ºå½¢çŠ¶ä¸º [num_mem_tokens, 1, hidden_size] çš„å¼ é‡
        # 1 æ˜¯ batch ç»´åº¦çš„å ä½ç¬¦ï¼Œä¼šåœ¨ä½¿ç”¨æ—¶ expand
        mem_tokens = torch.randn(self.config.num_mem_tokens, 1, hidden_size) * 0.02
        
        # æ³¨å†Œä¸ºå¯å­¦ä¹ å‚æ•°
        self.mem_tokens = nn.Parameter(mem_tokens, requires_grad=True)
```

**ä¸ºä»€ä¹ˆç”¨å°éšæœºåˆå§‹åŒ–ï¼Ÿ**
- é¿å…åˆå§‹å€¼è¿‡å¤§å½±å“è®­ç»ƒç¨³å®šæ€§
- è®©æ¨¡å‹ä»æ¥è¿‘é›¶çš„çŠ¶æ€å­¦ä¹ è®°å¿†è¡¨ç¤º
- 0.02 çš„æ ‡å‡†å·®æ˜¯ç»éªŒå€¼

---

### 4.2 è®°å¿†åµŒå…¥

```python
# åœ¨ embed_prefix ä¸­
if self.config.num_mem_tokens > 0:
    if mem_tokens is None:
        # ç¬¬ä¸€æ¬¡ä½¿ç”¨ï¼šä»åˆå§‹åŒ–çš„å‚æ•° expand
        mem_emb = self.mem_tokens.expand(-1, batch_size, -1)
    else:
        # åç»­ä½¿ç”¨ï¼šä½¿ç”¨ä¸Šä¸€æ—¶é—´æ­¥ä¼ æ¥çš„è®°å¿†
        mem_emb = mem_tokens
    
    # è½¬ç½®ï¼š[num_mem, batch, hidden] â†’ [batch, num_mem, hidden]
    mem_emb = mem_emb.transpose(0, 1)
    embs.append(mem_emb)
    
    # åˆ›å»ºæ©ç ï¼šè®°å¿† tokens éƒ½æ˜¯æœ‰æ•ˆçš„
    mem_mask = torch.ones(batch_size, self.config.num_mem_tokens, 
                          dtype=torch.bool, device=device)
    pad_masks.append(mem_mask)
    
    # æ³¨æ„åŠ›æ©ç ï¼š0 è¡¨ç¤ºå¯ä»¥æ³¨æ„
    att_masks += [0] * self.config.num_mem_tokens
```

---

### 4.3 è®°å¿†æå–

```python
# åœ¨ forward ä¸­
if self.config.num_mem_tokens > 0:
    if self.config.mem_at_end:
        # ä»åºåˆ—æœ«å°¾æå–
        updated_mem_tokens = prefix_out[:, -self.config.num_mem_tokens:, :]
    else:
        # ä»åºåˆ—å¼€å¤´æå–ï¼ˆæ¨èï¼‰
        updated_mem_tokens = prefix_out[:, :self.config.num_mem_tokens, :]
    
    # è½¬ç½®å›ï¼š[batch, num_mem, hidden] â†’ [num_mem, batch, hidden]
    updated_mem_tokens = updated_mem_tokens.transpose(0, 1)
```

**ä¸ºä»€ä¹ˆä»å¼€å¤´æå–ï¼Ÿ**
- è®°å¿† tokens åœ¨åºåˆ—å¼€å¤´
- ç»è¿‡ Transformer åï¼Œå®ƒä»¬çš„è¡¨ç¤ºè¢«æ›´æ–°
- æå–æ›´æ–°åçš„è¡¨ç¤ºä½œä¸ºä¸‹ä¸€æ—¶é—´æ­¥çš„è®°å¿†

---

### 4.4 è®°å¿†çŠ¶æ€ç®¡ç†

```python
class SmolVLAPolicy:
    def reset(self):
        """Episode å¼€å§‹æ—¶è°ƒç”¨"""
        self._mem_tokens_state = None  # æ¸…ç©ºè®°å¿†
    
    def _get_action_chunk(self, batch):
        # ä¼ å…¥å½“å‰è®°å¿†çŠ¶æ€
        actions, updated_mem = self.model.sample_actions(
            ..., mem_tokens=self._mem_tokens_state
        )
        
        # æ›´æ–°è®°å¿†çŠ¶æ€ï¼ˆä½¿ç”¨ detach é¿å…æ¢¯åº¦ç´¯ç§¯ï¼‰
        if self.config.num_mem_tokens > 0:
            self._mem_tokens_state = updated_mem.detach()
        
        return actions
```

**ä¸ºä»€ä¹ˆç”¨ detachï¼Ÿ**
- æ¨ç†æ—¶ä¸éœ€è¦æ¢¯åº¦
- é¿å…è®°å¿†çŠ¶æ€ç´¯ç§¯æ¢¯åº¦å¯¼è‡´å†…å­˜æ³„æ¼
- ä¿æŒè®°å¿†çŠ¶æ€çš„å€¼ï¼Œä½†åˆ‡æ–­è®¡ç®—å›¾

---

## 5. æ€»ç»“

### 5.1 æ ¸å¿ƒä¿®æ”¹

1. **é…ç½®å±‚**: æ·»åŠ  3 ä¸ªè®°å¿†ç›¸å…³å‚æ•°
2. **æ¨¡å‹å±‚**: 
   - åˆå§‹åŒ–å¯å­¦ä¹ è®°å¿† tokens
   - ä¿®æ”¹è¾“å…¥åµŒå…¥é€»è¾‘
   - ä¿®æ”¹å‰å‘ä¼ æ’­è¿”å›å€¼
3. **ç­–ç•¥å±‚**: 
   - ç®¡ç†è®°å¿†çŠ¶æ€
   - åœ¨æ—¶é—´æ­¥ä¹‹é—´ä¼ é€’è®°å¿†

### 5.2 ä¸ LM-RMT çš„ä¸»è¦åŒºåˆ«

| ç»´åº¦ | LM-RMT | SmolVLA Memory |
|------|--------|----------------|
| **åº”ç”¨** | è¯­è¨€å»ºæ¨¡ | æœºå™¨äººæ§åˆ¶ |
| **è¾“å…¥** | çº¯æ–‡æœ¬ | å¤šæ¨¡æ€ï¼ˆå›¾åƒ+è¯­è¨€+çŠ¶æ€ï¼‰ |
| **è¾“å‡º** | æ–‡æœ¬é¢„æµ‹ | åŠ¨ä½œé¢„æµ‹ |
| **è®°å¿†ç®¡ç†** | åœ¨æ¨¡å‹å†…éƒ¨ | åœ¨ç­–ç•¥å±‚ |
| **è®­ç»ƒ** | å¯è·¨ segment | æ¯æ ·æœ¬ç‹¬ç«‹ |
| **æ¨ç†** | Segment çº§åˆ« | æ—¶é—´æ­¥çº§åˆ« |

### 5.3 è®¾è®¡ä¼˜åŠ¿

1. **è½»é‡çº§**: ä»…å¢åŠ  0.0009% å‚æ•°
2. **çµæ´»**: å¯ä»¥å®Œå…¨ç¦ç”¨
3. **å…¼å®¹**: ä¸å½±å“ç°æœ‰åŠŸèƒ½
4. **é«˜æ•ˆ**: æ¨ç†å¼€é”€ < 2%
5. **å¯è§£é‡Š**: è®°å¿†çŠ¶æ€å¯ä»¥å¯è§†åŒ–

---

**å®ç°å®Œæˆï¼** ğŸ‰

è¿™ä¸ªå®ç°ä¿ç•™äº† RMT çš„æ ¸å¿ƒæ€æƒ³ï¼ˆå¯å­¦ä¹ è®°å¿† tokensï¼‰ï¼ŒåŒæ—¶é€‚é…äº† SmolVLA çš„å¤šæ¨¡æ€æ¶æ„å’Œæœºå™¨äººæ§åˆ¶åœºæ™¯ã€‚
